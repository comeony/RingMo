base_config: [
  '../base/context/default_mode.yaml',
  '../base/datasets/finetune_dataset.yaml',
  '../base/models/vit_base_p16.yaml',
  '../base/schedules/default_schedule.yaml',
  '../base/runner/runner.yaml',
  '../base/modelarts/aicc.yaml',
  '../base/__base__.yaml']

arch: "mae"
use_parallel: True
profile: False
auto_tune: False  # dataset performance
filepath_prefix: "./autotune"
autotune_per_step: 10

model:
    init_values:
    use_abs_pos_emb: True
    use_rel_pos_bias: False  # if set window size, it's True

finetune_dataset:
  data_type: "custom"
  train_path: "/data/imageNet-1k/train"
  eval_path: "/data/imageNet-1k/val"
  eval_interval: 1
  eval_offset: -1
  input_columns: [ "image", "label" ]
  output_columns: [ "image", "label" ]
  column_order: [ "image", "label" ]
  samples_num: 50000 # 50000 for imagenet, 25200 for nwpuï¼Œ 5000 for aid_test.

train_config:
  epoch: 100
  batch_size: 32
  image_size: 224
  per_epoch_size: 0
  num_classes: 1000
  resume_ckpt: "mae_pretrain_vit_base.ckpt"

loss_type: "soft_ce"

# optimizer
optimizer:
  optim_name: "AdamW"
  beta1: 0.9
  beta2: 0.999
  eps: 0.00000001 # 1e-8
  weight_decay: 0.05
  layer_decay: 0.65

# lr sechdule
lr_schedule:
  lr_type: "warmup_cosine_decay"
  base_lr: 0.0005
  min_lr: 0.000001
  warmup_lr: 0.
  warmup_epochs: 5

train_wrapper:
  use_clip_grad: False

callback:
    # ckpt callback
    ckpt_config:
        prefix: "finetune-mae-vit-base-p16"

aicc_config:
  obs_path: "Input your obs path if code is running on modelarts else invalid."
  upload_frequence: 1
  keep_last: False
